[[elasticsearch-ingest-data-through-logstash]]
= Logstash

// :description: Use {ls} to ship data to {es}.
// :keywords: serverless, elasticsearch, ingest, logstash, how to

{ls} is an open source data collection engine with real-time pipelining capabilities.
It supports a wide variety of data sources, and can dynamically unify data from disparate sources and normalize the data into destinations of your choice.

{ls} can collect data using a variety of {ls} {logstash-ref}/input-plugins.html[input plugins], enrich and transform the data with {ls} {logstash-ref}/filter-plugins.html[filter plugins],
and output the data to {es} using the {ls} {logstash-ref}/plugins-outputs-elasticsearch.html[{es} output plugin].

You can use {ls} to extend <<elasticsearch-ingest-data-through-beats,Beats>> for advanced use cases,
such as data routed to multiple destinations or when you need to make your data persistent.

.Logstash for {es-serverless}
[NOTE]
====
{ls} is a powerful, versatile ETL (Extract, Transform, Load) engine that can play an important role in organizations of all sizes.
Some capabilities and features for large, self-managed users aren't appropriate for {serverless-short}.

You'll use the {ls} {logstash-ref}/plugins-outputs-elasticsearch.html[{es} output plugin] to send data to {es3}.
Some differences to note between {es3} and self-managed {es}:

* Your logstash-output-elasticsearch configuration uses **API keys** to access {es} from {ls}.
User-based security settings are ignored and may cause errors.
* {es3} uses **{dlm} ({dlm-init})** instead of {ilm} ({ilm-init}).
If you add {ilm-init} settings to your {es} output configuration, they are ignored and may cause errors.
* **{ls} monitoring** for {serverless-short} is available through the https://github.com/elastic/integrations/blob/main/packages/logstash/_dev/build/docs/README.md[{ls} Integration] in <<what-is-observability-serverless,Elastic Observability>>.

**Known issue**

* The logstash-output-elasticsearch `hosts` setting defaults to port `:9200`.
Set the value to port `:443` instead.
====

[discrete]
[[elasticsearch-ingest-data-through-logstash-requirements]]
== Requirements

To use {ls} to send data to {es3}, you must be using:

* {ls} 8.10.1 or later
* {ls} {logstash-ref}/plugins-outputs-elasticsearch.html[{es} output plugin] 11.18.0 or later
* {ls} {logstash-ref}/plugins-inputs-elasticsearch.html[{es} input plugin] 4.18.0 or later
* {ls} {logstash-ref}/plugins-filters-elasticsearch.html[{es} filter plugin] 3.16.0 or later

[discrete]
[[elasticsearch-ingest-data-through-logstash-secure-connection]]
== Secure connection

{es-serverless} simplifies secure communication between {ls} and {es}.
Configure the {logstash-ref}/plugins-outputs-elasticsearch.html[{es} output] plugin to use
{logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-cloud_id[`cloud_id`] and
{logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-api_key[`api_key`].
No additional SSL configuration steps are needed.

[discrete]
[[elasticsearch-ingest-data-through-logstash-api-keys-for-connecting-ls-to-es3]]
== API keys for connecting {ls} to {es3}

Use the **Security: API key** section in the UI to <<api-keys,create an API key>>
for securely connecting the {ls} {es} output to {es3}.
We recommend creating a unique API key per {ls} instance.
You can create as many API keys as necessary.

When you set up your API keys, use the metadata option to tag each API key with details that are meaningful to you.
This step makes managing multiple API keys easier.

After you generate an API key, add it to your {ls} {logstash-ref}/plugins-outputs-elasticsearch.html[{es} output plugin] config file's `api_key` setting.
Here's an example:

[source,bash]
----
output {
  elasticsearch {
    api_key => "TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA"
  }
}
----

[discrete]
[[elasticsearch-ingest-data-through-logstash-migrating-elasticsearch-data-using-ls]]
== Migrating {es} data using {ls}

You can use {ls} to migrate data from self-managed {es} or {ess} to {es3}, or to migrate data from one {es3} deployment to another.

Create a {logstash-ref}/configuration.html[{ls} pipeline] that includes the {es} {logstash-ref}/plugins-inputs-elasticsearch.html[input plugin] and {logstash-ref}/plugins-outputs-elasticsearch.html[output plugin].

Configure the {es} input to point to your source deployment or instance, and configure the {es} output with the `cloud_id` and `api_key` settings for your target {es3} instance.

If your origin index is using <<elasticsearch-differences,settings that aren't supported in Serverless>>, then you might need to adjust your index settings.

[discrete]
[[elasticsearch-ingest-data-through-logstash-next-steps]]
== Next steps

Check out the https://www.elastic.co/logstash[Logstash product page] to see what {ls} can do for you.
When you're ready,
dive into the {logstash-ref}/index.html[Logstash documentation].
